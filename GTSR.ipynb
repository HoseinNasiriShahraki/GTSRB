{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"C:/Users/hosei/Documents/DataSets/GTSRB\"\n",
    "train_path = \"C:/Users/hosei/Documents/DataSets/GTSRB/Train/\"\n",
    "test_path = \"C:/Users/hosei/Documents/DataSets/GTSRB/Test/\"\n",
    "meta_path = \"C:/Users/hosei/Documents/DataSets/GTSRB/Meta/\"\n",
    "\n",
    "meta_df = pd.read_csv('C:/Users/hosei/Documents/DataSets/GTSRB/Meta.csv')\n",
    "train_df = pd.read_csv('C:/Users/hosei/Documents/DataSets/GTSRB/Train.csv')\n",
    "test_df = pd.read_csv('C:/Users/hosei/Documents/DataSets/GTSRB/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parametes\n",
    "num_of_classes = len(os.listdir(train_path))\n",
    "height = 30\n",
    "width = 30\n",
    "channels = 3\n",
    "n_inputs = height * width * num_of_classes\n",
    "num_epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #data Visualation\n",
    "# folder_names = [os.path.join(train_path, str(i)) for i in random.choices(range(43), k=20)] \n",
    "# file_names = [os.path.join(fldr, os.listdir(fldr)[0]) for fldr in folder_names]\n",
    "\n",
    "# fig, axes = plt.subplots(4, 5, figsize=(12, 8))\n",
    "# for i, image_path in enumerate(file_names):\n",
    "#     image = Image.open(image_path)\n",
    "#     row = i // 5\n",
    "#     col = i % 5\n",
    "#     axes[row, col].imshow(image)\n",
    "#     axes[row, col].axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Data ToTensor and Normalize it + resize to 28*28\n",
    "\n",
    "transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Resize((28,28)),\n",
    "    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "\n",
    "#creating the dataset class\n",
    "class GTSR_DataSet(Dataset):\n",
    "    def __init__(self, df, root_dir,transform=None):\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image_path = os.path.join(self.root_dir,self.df.iloc[index,7])  #the column of paths in dataframe is 7\n",
    "        image = Image.open(image_path)\n",
    "        y_class = torch.tensor(self.df.iloc[index, 6]) #the column of ClsassId in daraframe is 6\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            return (image, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = GTSR_DataSet(train_df,data_dir,transform=transforms)\n",
    "test_set = GTSR_DataSet(test_df,data_dir,transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': 39209, 'testing': 12630}\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset = training_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dataloaders = {'training':train_loader,'testing':test_loader}\n",
    "dataset_sizes = {'training':len(train_loader.dataset),'testing':len(test_loader.dataset)}\n",
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "class GTRSB_Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(GTRSB_Model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "        # self.conv6 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1)\n",
    "        # self.batchnorm3 = nn.BatchNorm2d(1024)\n",
    "        # self.maxpool3 = nn.AdaptiveMaxPool2d(512)\n",
    "\n",
    "        \n",
    "        self.l1 = nn.Linear(12544,512)\n",
    "        self.l2 = nn.Linear(512,128)\n",
    "        self.batchnorm4 = nn.LayerNorm(128)\n",
    "        self.l3 = nn.Linear(128,output_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,input):\n",
    "        #training pipeline\n",
    "\n",
    "        conv = self.conv1(input)\n",
    "        conv = self.conv2(conv)\n",
    "\n",
    "        batchnorm = self.relu(self.batchnorm1(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        conv = self.conv3(maxpool)\n",
    "        conv = self.conv4(conv)\n",
    "\n",
    "        batchnorm = self.relu(self.batchnorm2(conv))\n",
    "        maxpool = self.maxpool(batchnorm)\n",
    "\n",
    "        # conv = self.conv5(maxpool)\n",
    "        # conv = self.conv6(conv)\n",
    "        # batchnorm = self.relu(self.batchnorm3(conv))\n",
    "        # maxpool = self.maxpool3(batchnorm)\n",
    "    \n",
    "\n",
    "              \n",
    "        flatten = self.flatten(maxpool)\n",
    "        \n",
    "        #Neural Network Featuremap input\n",
    "        dense_l1 = self.l1(flatten)\n",
    "        dropout = self.dropout3(dense_l1)\n",
    "        dense_l2 = self.l2(dropout)\n",
    "        batchnorm = self.batchnorm4(dense_l2)\n",
    "        dropout = self.dropout2(batchnorm)\n",
    "        output = self.l3(dropout)\n",
    "        \n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GTRSB_Model(\n",
       "  (relu): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (dropout3): Dropout(p=0.3, inplace=False)\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (l1): Linear(in_features=12544, out_features=512, bias=True)\n",
       "  (l2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (batchnorm4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (l3): Linear(in_features=128, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 3*28*28\n",
    "output_size = 43\n",
    "model = GTRSB_Model(input_size=input_size, output_size=output_size)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 20, Adamax accuracy = 97.04671417260491\n",
      "epoch 1 / 20, NAdam accuracy = 95.13855898653999\n",
      "epoch 1 / 20, RAdam accuracy = 95.93032462391132\n",
      "epoch 1 / 20, RMSprop accuracy = 94.07759303246239\n",
      "epoch 1 / 20, Rprop accuracy = 97.11005542359462\n"
     ]
    }
   ],
   "source": [
    "# model deployment with SGD Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate , momentum=0.9)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, SGD accuracy = {acc}')\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "# model deployment with Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, ADAM accuracy = {acc}')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "# model deployment with ASGD Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.ASGD(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, ASGD accuracy = {acc}')\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model deployment with Adadelta Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate , rho=0.9)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, Adadelta accuracy = {acc}')\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "# model deployment with Adagrad Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, Adagrad accuracy = {acc}')\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "        \n",
    "        \n",
    "# model deployment with AdamW Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, AdamW accuracy = {acc}')\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# model deployment with Adamax Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, Adamax accuracy = {acc}')\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"---------------------------------\")        \n",
    "\n",
    "        \n",
    "# model deployment with NAdam Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, NAdam accuracy = {acc}')\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "        \n",
    "# model deployment with RAdam Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, RAdam accuracy = {acc}')\n",
    "        \n",
    "        \n",
    "        \n",
    "print(\"---------------------------------\")\n",
    "\n",
    "        \n",
    "# model deployment with RMSprop Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, RMSprop accuracy = {acc}')\n",
    "        \n",
    "\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "        \n",
    "        \n",
    "# model deployment with Rprop Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Rprop(model.parameters(), lr=learning_rate)\n",
    "for epoch in range(1):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # images, labels = images.type(torch.LongTensor), labels.type(torch.LongTensor)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if (i+1) % 500 == 0:\n",
    "        #     print(f'epoch {epoch+1} / {num_epochs}, step {i+1}, loss = {loss.item()}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.type(torch.cuda.FloatTensor), labels.type(torch.cuda.FloatTensor)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            n_samples +=labels.shape[0]\n",
    "            n_correct +=(predictions == labels).sum().item()\n",
    "        acc = 100.0 * (n_correct / n_samples)\n",
    "        print(f'epoch {epoch+1} / 20, Rprop accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
